{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "model_name_or_path = \"openai/whisper-large-v3-turbo\"\n",
    "language = \"English\"\n",
    "language_abbr = \"en\"\n",
    "task = \"transcribe\"\n",
    "dataset_name = \"/\"\n",
    "\n",
    "org = \"\"\n",
    "trained_adapter_name = \"whisper-turbo-names-adapters\"\n",
    "trained_model_name = \"whisper-turbo-names\"\n",
    "\n",
    "trained_adapter_repo = org + \"/\" + trained_adapter_name\n",
    "trained_model_repo = org + \"/\" + trained_model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ronak\\anaconda3\\envs\\s2s_project\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import (\n",
    "    AutomaticSpeechRecognitionPipeline,\n",
    "    WhisperTimeStampLogitsProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_asr = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model_name_or_path,\n",
    "    chunk_length_s=30,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def format_time(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    seconds = int(seconds % 60)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
    "\n",
    "def process_audio_create_vtt(audio_filename, audio_type, whisper_asr, output_filename=None):\n",
    "    prediction = whisper_asr(f\"{audio_filename}.{audio_type}\", return_timestamps=True)\n",
    "    \n",
    "    vtt_filename = output_filename if output_filename else f\"{audio_filename}.vtt\"\n",
    "\n",
    "    with open(vtt_filename, \"w\", encoding=\"utf-8\") as vtt_file:\n",
    "        vtt_file.write(\"WEBVTT\\n\\n\")\n",
    "\n",
    "        for i, chunk in enumerate(prediction.get(\"chunks\", [])):\n",
    "            start, end = chunk.get(\"timestamp\", (None, None))\n",
    "            text = chunk.get(\"text\", \"\").strip()\n",
    "\n",
    "            if start is None or end is None or not text:\n",
    "                continue\n",
    "\n",
    "            start_time = format_time(start)\n",
    "            end_time = format_time(end)\n",
    "\n",
    "            if not re.match(r\"^\\d{2}:\\d{2}:\\d{2}\\.\\d{3}$\", start_time) or not re.match(r\"^\\d{2}:\\d{2}:\\d{2}$\", end_time):\n",
    "                continue\n",
    "            vtt_file.write(f\"{i+1}\\n{start_time}.000 --> {end_time}.000\\n{text}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 109\u001b[0m\n\u001b[0;32m    101\u001b[0m     dataset_dict \u001b[38;5;241m=\u001b[39m DatasetDict({\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_dataset, \n\u001b[0;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m: valid_dataset\n\u001b[0;32m    104\u001b[0m     })\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset_dict\n\u001b[1;32m--> 109\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_audio_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_vtt_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_audio_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_vtt_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m dataset\u001b[38;5;241m.\u001b[39msave_to_disk(save_path)\n\u001b[0;32m    113\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mcast_column(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m, Audio())\n",
      "Cell \u001b[1;32mIn[5], line 94\u001b[0m, in \u001b[0;36mcreate_dataset\u001b[1;34m(train_audio_file, train_vtt_file, validation_audio_file, validation_vtt_file, save_path)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_dataset\u001b[39m(train_audio_file, train_vtt_file, validation_audio_file, validation_vtt_file, save_path):\n\u001b[0;32m     92\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(save_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 94\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_audio_create_vtt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_audio_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_vtt_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msave_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     validation_data \u001b[38;5;241m=\u001b[39m process_audio_create_vtt(validation_audio_file, validation_vtt_file, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/validation.vtt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     98\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict(transform_data(train_data))\n",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m, in \u001b[0;36mprocess_audio_create_vtt\u001b[1;34m(audio_filename, audio_type, whisper_asr, output_filename)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_audio_create_vtt\u001b[39m(audio_filename, audio_type, whisper_asr, output_filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 10\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mwhisper_asr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43maudio_filename\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43maudio_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     vtt_filename \u001b[38;5;241m=\u001b[39m output_filename \u001b[38;5;28;01mif\u001b[39;00m output_filename \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.vtt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(vtt_filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m vtt_file:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, Audio, Dataset\n",
    "import webvtt\n",
    "from datetime import datetime\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# setup\n",
    "hf_username = \"\"\n",
    "repo_name = \"names\"\n",
    "train_audio_file = \"\"\n",
    "train_vtt_file = \"\"\n",
    "validation_audio_file = \"\"\n",
    "validation_vtt_file = \"\"\n",
    "save_path = f\"data/{repo_name}-dataset\"\n",
    "\n",
    "def parse_time(time_str):\n",
    "    return datetime.strptime(time_str, \"%H:%M:%S.%f\")\n",
    "\n",
    "def milliseconds(time_obj):\n",
    "    return time_obj.hour * 3600000 + time_obj.minute * 60000 + time_obj.second * 1000 + time_obj.microsecond // 1000\n",
    "\n",
    "def time_to_samples(time_ms, sr):\n",
    "    return int(time_ms * sr / 1000)\n",
    "\n",
    "def transform_data(data):\n",
    "    audio_path = data[\"train_audio_file\"]\n",
    "    vtt_path = data[\"train_vtt_file\"]\n",
    "    output_dir = data[\"save_path\"]\n",
    "\n",
    "    full_audio, sr = librosa.load(audio_path, sr=None, mono=True)\n",
    "\n",
    "    captions = webvtt.read(vtt_path)\n",
    "    data = []\n",
    "    current_text = []\n",
    "    current_start = None\n",
    "    current_end = None\n",
    "    accumulated_duration = 0\n",
    "    segment_counter = 0\n",
    "\n",
    "    for caption in captions:\n",
    "        start = milliseconds(parse_time(caption.start))\n",
    "        end = milliseconds(parse_time(caption.end))\n",
    "        duration = (end - start).total_seconds()\n",
    "\n",
    "        if current_start is None:\n",
    "            current_start = start\n",
    "\n",
    "        if accumulated_duration + duration > 30:\n",
    "            current_text.append(caption.text)\n",
    "            current_end = end\n",
    "            accumulated_duration += duration\n",
    "        \n",
    "        else:\n",
    "            segment_filename = f\"{output_dir}/segment_{segment_counter}.mp3\"\n",
    "            start_samples = time_to_samples(milliseconds(current_start.time()), sr)\n",
    "            end_samples = time_to_samples(milliseconds(current_end.time()), sr)\n",
    "            audio_segment = full_audio[start_samples:end_samples]\n",
    "            sf.write(segment_filename, audio_segment, sr, format=\"mp3\")\n",
    "\n",
    "            data.append({\n",
    "                \"audio\": segment_filename,\n",
    "                \"text\": \" \".join(current_text),\n",
    "                \"start_time\": current_start.strftime(\"%H:%M:%S.%f\")[:-3],\n",
    "                \"end_time\": current_end.strftime(\"%H:%M:%S.%f\")[:-3],\n",
    "            })\n",
    "\n",
    "            current_text = [caption.text]\n",
    "            current_start = start\n",
    "            current_end = end\n",
    "            accumulated_duration = duration\n",
    "            segment_counter += 1\n",
    "    \n",
    "    if current_text:\n",
    "        segment_filename = f\"{output_dir}/segment_{segment_counter}.mp3\"\n",
    "        start_samples = time_to_samples(milliseconds(current_start.time()), sr)\n",
    "        end_samples = time_to_samples(milliseconds(current_end.time()), sr)\n",
    "        audio_segment = full_audio[start_samples:end_samples]\n",
    "        sf.write(segment_filename, audio_segment, sr, format=\"mp3\")\n",
    "\n",
    "        data.append({\n",
    "            \"audio\": segment_filename,\n",
    "            \"text\": \" \".join(current_text),\n",
    "            \"start_time\": current_start.strftime(\"%H:%M:%S.%f\")[:-3],\n",
    "            \"end_time\": current_end.strftime(\"%H:%M:%S.%f\")[:-3],\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_dataset(train_audio_file, train_vtt_file, validation_audio_file, validation_vtt_file, save_path):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    train_data = process_audio_create_vtt(train_audio_file, train_vtt_file, f\"{save_path}\")\n",
    "\n",
    "    validation_data = process_audio_create_vtt(validation_audio_file, validation_vtt_file, f\"{save_path}/validation.vtt\")\n",
    "\n",
    "    train_dataset = Dataset.from_dict(transform_data(train_data))\n",
    "    valid_dataset = Dataset.from_dict(transform_data(validation_data))\n",
    "\n",
    "    dataset_dict = DatasetDict({\n",
    "        \"train\": train_dataset, \n",
    "        \"validation\": valid_dataset\n",
    "    })\n",
    "\n",
    "    return dataset_dict\n",
    "\n",
    "\n",
    "dataset = create_dataset(train_audio_file, train_vtt_file, validation_audio_file, validation_vtt_file, save_path)\n",
    "\n",
    "dataset.save_to_disk(save_path)\n",
    "\n",
    "dataset = Dataset.cast_column(\"audio\", Audio())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, DatasetDict\n\u001b[0;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m DatasetDict()\n\u001b[1;32m----> 4\u001b[0m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m load_dataset(dataset_name, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset)\n",
      "File \u001b[1;32mc:\\Users\\ronak\\anaconda3\\envs\\s2s_project\\lib\\site-packages\\datasets\\load.py:2132\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2127\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[0;32m   2128\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[0;32m   2129\u001b[0m )\n\u001b[0;32m   2131\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 2132\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   2133\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m   2134\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   2135\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m   2136\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[0;32m   2137\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2138\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m   2139\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m   2140\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   2141\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   2142\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2143\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   2144\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2145\u001b[0m     _require_default_config_name\u001b[38;5;241m=\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2146\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   2147\u001b[0m )\n\u001b[0;32m   2149\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   2150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32mc:\\Users\\ronak\\anaconda3\\envs\\s2s_project\\lib\\site-packages\\datasets\\load.py:1853\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[0;32m   1851\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[0;32m   1852\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[1;32m-> 1853\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1862\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1864\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[1;32mc:\\Users\\ronak\\anaconda3\\envs\\s2s_project\\lib\\site-packages\\datasets\\load.py:1534\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[0;32m   1531\u001b[0m download_config\u001b[38;5;241m.\u001b[39mforce_extract \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1532\u001b[0m download_config\u001b[38;5;241m.\u001b[39mforce_download \u001b[38;5;241m=\u001b[39m download_mode \u001b[38;5;241m==\u001b[39m DownloadMode\u001b[38;5;241m.\u001b[39mFORCE_REDOWNLOAD\n\u001b[1;32m-> 1534\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1536\u001b[0m     filename \u001b[38;5;241m=\u001b[39m filename \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "dataset = DatasetDict()\n",
    "dataset[\"train\"] = load_dataset(dataset_name, split=\"train\")\n",
    "dataset[\"validation\"] = load_dataset(dataset_name, split=\"validation\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\ronak\\anaconda3\\envs\\s2s_project\\lib\\site-packages\\datasets\\dataset_dict.py:72\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[1;34m(self, k)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     75\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m     76\u001b[0m         ]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'train'"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    batch[\"input_features\"] = feature_extractor(audio, sampling_rate=audio.sampling_rate)\n",
    "\n",
    "    batch[\"labels\"] = tokenizer(batch[\"text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        \n",
    "        if (labels[:, 0] == self.processor.tokenizer.pad_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path)\n",
    "model.config.forced_decoder_ids = None  # Optional: Let the model generate freely\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhisperForConditionalGeneration(\n",
      "  (model): WhisperModel(\n",
      "    (encoder): WhisperEncoder(\n",
      "      (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (embed_positions): Embedding(1500, 1280)\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x WhisperEncoderLayer(\n",
      "          (self_attn): WhisperSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): WhisperDecoder(\n",
      "      (embed_tokens): Embedding(51866, 1280, padding_idx=50257)\n",
      "      (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x WhisperDecoderLayer(\n",
      "          (self_attn): WhisperSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): WhisperSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (proj_out): Linear(in_features=1280, out_features=51866, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 27,852,800 || all params: 836,730,880 || trainable%: 3.3288\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(r=32, lora_alpha=8, use_rslora=True,\n",
    "                    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n",
    "                    lora_dropout=0.05, bias =\"none\")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    per_device_eval_batch_size=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=5e-4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    bf16=True,\n",
    "    generation_max_length=128,\n",
    "    logging_steps=2,\n",
    "    save_steps=2,\n",
    "    eval_steps=2,\n",
    "    remove_unused_columns=False,\n",
    "    lr_scheduler_type=\"cosntant\",\n",
    "    warmup_steps=2,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=2,\n",
    "    overwrite_output_dir=True,\n",
    "    logging_dir=\"logs\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2s_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
